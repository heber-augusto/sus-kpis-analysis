{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNviCQI6IDqh"
      },
      "source": [
        "\n",
        "# I - Configuração do ambiente\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nqola1AcrdGm"
      },
      "source": [
        "## 1.1 - Montagem do drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GYbxgrEQz3Sn"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enmrocYf_raz",
        "outputId": "0b0b36c1-0f2f-4fba-e9f4-3cb0f7c8cedc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnBLAcn1ZEik"
      },
      "source": [
        "## 1.2 - Escolha do ambiente e carregamento\n",
        "São disponibilizadas duas opções de execução (dependendo da credencial utilizada)\n",
        " - Leitura: disponível para analises e usuários que não possuem permissão de escrita;\n",
        " - Escrita: disponível para atualização de bases (testes de desenvolvimento ou ambiente de produção)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "Fj3xNzUzZD8W",
        "outputId": "4b8cd6bc-bac4-49e9-c989-f34e68855889"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: SERVICE_ACCOUNT_USER=''\n",
            "env: SERVICE_ACCOUNT_JSON=''\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'shared_drive'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import os\n",
        "if os.path.isfile('/content/monitor-rosa-leitura.json'):\n",
        "    datalake_mode = 'leitura'\n",
        "    %env SERVICE_ACCOUNT_USER=acesso-leitura@monitor-rosa.iam.gserviceaccount.com\n",
        "    %env SERVICE_ACCOUNT_JSON=/content/monitor-rosa-leitura.json\n",
        "elif os.path.isfile('/content/monitor-rosa-escrita.json'):\n",
        "    datalake_mode = 'escrita'\n",
        "    %env SERVICE_ACCOUNT_USER=acesso-escrita@monitor-rosa.iam.gserviceaccount.com\n",
        "    %env SERVICE_ACCOUNT_JSON=/content/monitor-rosa-escrita.json\n",
        "else:\n",
        "    assert(os.path.isdir('/content/drive/Shareddrives/monitor-rosa-gold') == True)\n",
        "    datalake_mode = 'shared_drive'\n",
        "    %env SERVICE_ACCOUNT_USER=''\n",
        "    %env SERVICE_ACCOUNT_JSON=''\n",
        "datalake_mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxl5dgNHuCsK",
        "outputId": "ef06682d-6964-4a32-c619-b435fe1c76c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'sus-kpis-analysis': No such file or directory\n",
            "Cloning into 'sus-kpis-analysis'...\n",
            "remote: Enumerating objects: 1768, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 1768 (delta 16), reused 12 (delta 12), pack-reused 1740 (from 3)\u001b[K\n",
            "Receiving objects: 100% (1768/1768), 4.78 MiB | 21.58 MiB/s, done.\n",
            "Resolving deltas: 100% (816/816), done.\n"
          ]
        }
      ],
      "source": [
        "!rm -r sus-kpis-analysis\n",
        "!git clone https://github.com/heber-augusto/sus-kpis-analysis.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NYsSZynDK9K"
      },
      "source": [
        "## 1.3 - Instalação de libs Python, inicialização de variáveis de ambiente e configuração/instalação do Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6v3Wqjxhu4Gq",
        "outputId": "e1b3b5e6-a49e-43ec-f273-0d57098e851f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting findspark (from -r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 1))\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (from -r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 2)) (3.5.3)\n",
            "Collecting imagehash (from -r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 3))\n",
            "  Downloading ImageHash-4.3.1-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting delta-spark==2.4.0 (from -r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 4))\n",
            "  Downloading delta_spark-2.4.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.10/dist-packages (from -r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (2.19.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from -r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 6)) (2024.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from -r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 7)) (5.9.5)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from -r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 8)) (2.155.0)\n",
            "Collecting pyspark (from -r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 2))\n",
            "  Downloading pyspark-3.4.4.tar.gz (311.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.4/311.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: importlib-metadata>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from delta-spark==2.4.0->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 4)) (8.5.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 2)) (0.10.9.7)\n",
            "Collecting PyWavelets (from imagehash->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 3))\n",
            "  Downloading pywavelets-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from imagehash->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 3)) (1.26.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from imagehash->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 3)) (11.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from imagehash->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 3)) (1.13.1)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=2.26.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (2.27.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (2.19.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (2.7.2)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (2.32.3)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (1.6.0)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 8)) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 8)) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 8)) (4.1.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (1.66.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (4.25.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (1.25.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (4.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 8)) (3.2.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=1.0.0->delta-spark==2.4.0->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 4)) (3.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (2024.12.14)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.26.1->google-cloud-storage->-r /content/sus-kpis-analysis/sia/etls/requirements.txt (line 5)) (0.6.1)\n",
            "Downloading delta_spark-2.4.0-py3-none-any.whl (20 kB)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Downloading ImageHash-4.3.1-py2.py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.5/296.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pywavelets-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.4-py2.py3-none-any.whl size=311905460 sha256=50c9bb5710b1d031dcb5b648b275f2b50c9db646e23c4ee48a26fa8f87b93660\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/b2/e2/382460beb13a6ab29568e5e7807561604442b9556b385771ba\n",
            "Successfully built pyspark\n",
            "Installing collected packages: findspark, PyWavelets, pyspark, imagehash, delta-spark\n",
            "  Attempting uninstall: pyspark\n",
            "    Found existing installation: pyspark 3.5.3\n",
            "    Uninstalling pyspark-3.5.3:\n",
            "      Successfully uninstalled pyspark-3.5.3\n",
            "Successfully installed PyWavelets-1.8.0 delta-spark-2.4.0 findspark-2.0.1 imagehash-4.3.1 pyspark-3.4.4\n",
            "env: PYTHONHASHSEED=1234\n",
            "env: JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n",
            "env: SPARK_HOME=/content/spark-3.4.4-bin-hadoop3\n",
            "env: SPARK_VERSION=3.4.4\n",
            "starting spark env setup \n",
            "installing and downloading packages\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "setting enviroment variables\n",
            "spark env setup completed with success\n"
          ]
        }
      ],
      "source": [
        "!pip install -r /content/sus-kpis-analysis/sia/etls/requirements.txt\n",
        "\n",
        "%env PYTHONHASHSEED=1234\n",
        "%env JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n",
        "%env SPARK_HOME=/content/spark-3.4.4-bin-hadoop3\n",
        "%env SPARK_VERSION=3.4.4\n",
        "\n",
        "!source /content/sus-kpis-analysis/sia/etls/bin/setup_spark_env.sh '/content/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-_O3K75fMh7K"
      },
      "outputs": [],
      "source": [
        "if datalake_mode != 'shared_drive':\n",
        "    %env XDG_CONFIG_HOME=/content/datalake\n",
        "    !source /content/sus-kpis-analysis/sia/etls/bin/install-google-drive-ocamlfuse.sh\n",
        "    !source /content/sus-kpis-analysis/sia/etls/bin/mount_google_drive_v2.sh '/content/datalake' $SERVICE_ACCOUNT_USER '0ABIY-a4qrdY9Uk9PVA' 'monitor-rosa-bronze' $SERVICE_ACCOUNT_JSON '/content'\n",
        "    !source /content/sus-kpis-analysis/sia/etls/bin/mount_google_drive_v2.sh '/content/datalake' $SERVICE_ACCOUNT_USER '0ALl0owLNr53oUk9PVA' 'monitor-rosa-silver' $SERVICE_ACCOUNT_JSON '/content'\n",
        "    !source /content/sus-kpis-analysis/sia/etls/bin/mount_google_drive_v2.sh '/content/datalake' $SERVICE_ACCOUNT_USER '0AMHp9pBeLvZiUk9PVA' 'monitor-rosa-gold' $SERVICE_ACCOUNT_JSON '/content'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT00Lo1mTJwu"
      },
      "source": [
        "## 1.4 - Inicializa variáveis de acesso ao delta lake criado no drive\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> O caminho do warehouse pode ser alterado em caso de testes de escritas locais.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LLT8gGOcekpW",
        "outputId": "4ee6c661-ec18-4620-bb2b-5939af5a2037"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/spark-3.4.4-bin-hadoop3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "lake_prefix = \"temp-output\"\n",
        "\n",
        "if datalake_mode in ('leitura','shared_drive',):\n",
        "    warehouse_dir = f\"/content/datalake/{lake_prefix}/\"\n",
        "\n",
        "if datalake_mode == 'escrita':\n",
        "    warehouse_dir = f\"/content/datalake/\"\n",
        "\n",
        "spark_path = os.getenv('SPARK_HOME')\n",
        "spark_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpzMUcNg3k8d"
      },
      "source": [
        "## 1.5 - Inclusão da pasta do repositório no python path\n",
        "\n",
        "Procedimento permite que funções e classes presentes no repositório sejam utilizadas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-dOrv8t0IMZ",
        "outputId": "c4cb466c-12a1-491b-d901-c7c2aeb8584c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content',\n",
              " '/env/python',\n",
              " '/usr/lib/python310.zip',\n",
              " '/usr/lib/python3.10',\n",
              " '/usr/lib/python3.10/lib-dynload',\n",
              " '',\n",
              " '/usr/local/lib/python3.10/dist-packages',\n",
              " '/usr/lib/python3/dist-packages',\n",
              " '/usr/local/lib/python3.10/dist-packages/IPython/extensions',\n",
              " '/usr/local/lib/python3.10/dist-packages/setuptools/_vendor',\n",
              " '/root/.ipython',\n",
              " '/content/sus-kpis-analysis']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append('/content/sus-kpis-analysis')\n",
        "sys.path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptGWtMYSIXtw"
      },
      "source": [
        "## 1.6 - Importação de funções utilizadas pelo código"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "U0LNG7YrUyRD"
      },
      "outputs": [],
      "source": [
        "from sia.etls.lib.catalog_loader import DeltaLakeDatabaseFsCreator, load_entire_catalog_fs_v2\n",
        "from sia.etls.lib.table_utilities import vacuum_tables_from_database, table_exists\n",
        "from sia.etls.lib.fs_spark_session import create_fs_spark_session\n",
        "from sia.etls.lib.bronze_files_utilities import get_pending_files_from_bronze\n",
        "from sia.etls.lib.delta_table_creators import ParquetToDelta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LWOtjtk33d6"
      },
      "source": [
        "## 1.7 - Cria Sessão Spark conectada ao Delta Lake presente no Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ETM5T_gkDvAP"
      },
      "outputs": [],
      "source": [
        "spark = create_fs_spark_session(\n",
        "    warehouse_dir=warehouse_dir,\n",
        "    spark_path=spark_path\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8lBJqvC4Cak"
      },
      "source": [
        "## 1.8 - Refresh do catálogo para utilizar consultas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvYjAMOAyNmj",
        "outputId": "16443f06-6ffc-4078-9cf0-442c87ae380c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sia_bronze.db', 'cnes_bronze.db', 'sih_bronze.db', 'sim_bronze.db']\n",
            "Banco de dados sia_bronze criado.\n",
            "listando conteúdos do caminho /content/drive/Shareddrives/monitor-rosa-bronze/databases e database sia_bronze\n",
            "prefix: /content/drive/Shareddrives/monitor-rosa-bronze/databases/sia_bronze.db/\n",
            "table_list: ['ar', 'aq', 'pa', 'bi', 'am']\n",
            "Tabela ar criada\n",
            "Tabela ar criada com comando CREATE TABLE IF NOT EXISTS sia_bronze.ar USING delta LOCATION '/content/drive/Shareddrives/monitor-rosa-bronze/databases/sia_bronze.db/ar'\n",
            "Tabela aq criada\n",
            "Tabela aq criada com comando CREATE TABLE IF NOT EXISTS sia_bronze.aq USING delta LOCATION '/content/drive/Shareddrives/monitor-rosa-bronze/databases/sia_bronze.db/aq'\n",
            "Tabela pa criada\n",
            "Tabela pa criada com comando CREATE TABLE IF NOT EXISTS sia_bronze.pa USING delta LOCATION '/content/drive/Shareddrives/monitor-rosa-bronze/databases/sia_bronze.db/pa'\n",
            "Tabela bi criada\n",
            "Tabela bi criada com comando CREATE TABLE IF NOT EXISTS sia_bronze.bi USING delta LOCATION '/content/drive/Shareddrives/monitor-rosa-bronze/databases/sia_bronze.db/bi'\n",
            "Tabela am criada\n",
            "Tabela am criada com comando CREATE TABLE IF NOT EXISTS sia_bronze.am USING delta LOCATION '/content/drive/Shareddrives/monitor-rosa-bronze/databases/sia_bronze.db/am'\n",
            "Recriação das tabelas concluída.\n",
            "Banco de dados cnes_bronze criado.\n",
            "listando conteúdos do caminho /content/drive/Shareddrives/monitor-rosa-bronze/databases e database cnes_bronze\n",
            "prefix: /content/drive/Shareddrives/monitor-rosa-bronze/databases/cnes_bronze.db/\n",
            "table_list: ['dc', 'lt', 'ep', 'pf', 'eq', 'hb', 'in', 'rc', 'sr']\n",
            "Tabela dc criada\n",
            "Tabela dc criada com comando CREATE TABLE IF NOT EXISTS cnes_bronze.dc USING delta LOCATION '/content/drive/Shareddrives/monitor-rosa-bronze/databases/cnes_bronze.db/dc'\n",
            "Tabela lt criada\n",
            "Tabela lt criada com comando CREATE TABLE IF NOT EXISTS cnes_bronze.lt USING delta LOCATION '/content/drive/Shareddrives/monitor-rosa-bronze/databases/cnes_bronze.db/lt'\n",
            "Tabela ep criada\n",
            "Tabela ep criada com comando CREATE TABLE IF NOT EXISTS cnes_bronze.ep USING delta LOCATION '/content/drive/Shareddrives/monitor-rosa-bronze/databases/cnes_bronze.db/ep'\n",
            "Tabela pf criada\n",
            "Tabela pf criada com comando CREATE TABLE IF NOT EXISTS cnes_bronze.pf USING delta LOCATION '/content/drive/Shareddrives/monitor-rosa-bronze/databases/cnes_bronze.db/pf'\n",
            "Tabela eq criada\n",
            "Tabela eq criada com comando CREATE TABLE IF NOT EXISTS cnes_bronze.eq USING delta LOCATION '/content/drive/Shareddrives/monitor-rosa-bronze/databases/cnes_bronze.db/eq'\n",
            "Tabela hb criada\n",
            "Tabela hb criada com comando CREATE TABLE IF NOT EXISTS cnes_bronze.hb USING delta LOCATION '/content/drive/Shareddrives/monitor-rosa-bronze/databases/cnes_bronze.db/hb'\n",
            "Tabela in criada\n",
            "Tabela in criada com comando CREATE TABLE IF NOT EXISTS cnes_bronze.in USING delta LOCATION '/content/drive/Shareddrives/monitor-rosa-bronze/databases/cnes_bronze.db/in'\n",
            "Tabela rc criada\n",
            "Tabela rc criada com comando CREATE TABLE IF NOT EXISTS cnes_bronze.rc USING delta LOCATION '/content/drive/Shareddrives/monitor-rosa-bronze/databases/cnes_bronze.db/rc'\n",
            "Tabela sr criada\n",
            "Tabela sr criada com comando CREATE TABLE IF NOT EXISTS cnes_bronze.sr USING delta LOCATION '/content/drive/Shareddrives/monitor-rosa-bronze/databases/cnes_bronze.db/sr'\n",
            "Recriação das tabelas concluída.\n",
            "Banco de dados sih_bronze criado.\n",
            "listando conteúdos do caminho /content/drive/Shareddrives/monitor-rosa-bronze/databases e database sih_bronze\n",
            "prefix: /content/drive/Shareddrives/monitor-rosa-bronze/databases/sih_bronze.db/\n",
            "table_list: ['rd']\n",
            "Tabela rd criada\n",
            "Tabela rd criada com comando CREATE TABLE IF NOT EXISTS sih_bronze.rd USING delta LOCATION '/content/drive/Shareddrives/monitor-rosa-bronze/databases/sih_bronze.db/rd'\n",
            "Recriação das tabelas concluída.\n",
            "Banco de dados sim_bronze criado.\n",
            "listando conteúdos do caminho /content/drive/Shareddrives/monitor-rosa-bronze/databases e database sim_bronze\n",
            "prefix: /content/drive/Shareddrives/monitor-rosa-bronze/databases/sim_bronze.db/\n",
            "table_list: ['dores']\n",
            "Tabela dores criada\n",
            "Tabela dores criada com comando CREATE TABLE IF NOT EXISTS sim_bronze.dores USING delta LOCATION '/content/drive/Shareddrives/monitor-rosa-bronze/databases/sim_bronze.db/dores'\n",
            "Recriação das tabelas concluída.\n",
            "['cancer_data.db', 'ibge_silver.db']\n",
            "Banco de dados cancer_data criado.\n",
            "listando conteúdos do caminho /content/drive/Shareddrives/monitor-rosa-silver/databases e database cancer_data\n",
            "prefix: /content/drive/Shareddrives/monitor-rosa-silver/databases/cancer_data.db/\n",
            "table_list: ['aq_filtered', 'ar_filtered', 'dados_estados_mensal', 'dados_municipios_mensal', 'pacientes', 'procedimentos', 'procedimentos_e_pacientes', 'demografia_municipios', 'cadastro_municipios']\n",
            "Tabela aq_filtered criada\n",
            "Tabela aq_filtered criada com comando CREATE TABLE IF NOT EXISTS cancer_data.aq_filtered USING delta LOCATION '/content/drive/Shareddrives/monitor-rosa-silver/databases/cancer_data.db/aq_filtered'\n",
            "Tabela ar_filtered criada\n",
            "Tabela ar_filtered criada com comando CREATE TABLE IF NOT EXISTS cancer_data.ar_filtered USING delta LOCATION '/content/drive/Shareddrives/monitor-rosa-silver/databases/cancer_data.db/ar_filtered'\n",
            "Tabela dados_estados_mensal criada\n",
            "Tabela dados_estados_mensal criada com comando CREATE TABLE IF NOT EXISTS cancer_data.dados_estados_mensal USING delta LOCATION '/content/drive/Shareddrives/monitor-rosa-silver/databases/cancer_data.db/dados_estados_mensal'\n",
            "Tabela dados_municipios_mensal criada\n",
            "Tabela dados_municipios_mensal criada com comando CREATE TABLE IF NOT EXISTS cancer_data.dados_municipios_mensal USING delta LOCATION '/content/drive/Shareddrives/monitor-rosa-silver/databases/cancer_data.db/dados_municipios_mensal'\n",
            "Tabela pacientes criada\n",
            "Tabela pacientes criada com comando CREATE TABLE IF NOT EXISTS cancer_data.pacientes USING delta LOCATION '/content/drive/Shareddrives/monitor-rosa-silver/databases/cancer_data.db/pacientes'\n",
            "Tabela procedimentos criada\n",
            "Tabela procedimentos criada com comando CREATE TABLE IF NOT EXISTS cancer_data.procedimentos USING delta LOCATION '/content/drive/Shareddrives/monitor-rosa-silver/databases/cancer_data.db/procedimentos'\n",
            "Tabela procedimentos_e_pacientes criada\n",
            "Tabela procedimentos_e_pacientes criada com comando CREATE TABLE IF NOT EXISTS cancer_data.procedimentos_e_pacientes USING delta LOCATION '/content/drive/Shareddrives/monitor-rosa-silver/databases/cancer_data.db/procedimentos_e_pacientes'\n",
            "Tabela demografia_municipios criada\n",
            "Tabela demografia_municipios criada com comando CREATE TABLE IF NOT EXISTS cancer_data.demografia_municipios USING delta LOCATION '/content/drive/Shareddrives/monitor-rosa-silver/databases/cancer_data.db/demografia_municipios'\n",
            "Tabela cadastro_municipios criada\n",
            "Tabela cadastro_municipios criada com comando CREATE TABLE IF NOT EXISTS cancer_data.cadastro_municipios USING delta LOCATION '/content/drive/Shareddrives/monitor-rosa-silver/databases/cancer_data.db/cadastro_municipios'\n",
            "Recriação das tabelas concluída.\n",
            "Banco de dados ibge_silver criado.\n",
            "listando conteúdos do caminho /content/drive/Shareddrives/monitor-rosa-silver/databases e database ibge_silver\n",
            "prefix: /content/drive/Shareddrives/monitor-rosa-silver/databases/ibge_silver.db/\n",
            "table_list: ['cadastro_municipios', 'demografia_municipios']\n",
            "Tabela cadastro_municipios criada\n",
            "Tabela cadastro_municipios criada com comando CREATE TABLE IF NOT EXISTS ibge_silver.cadastro_municipios USING delta LOCATION '/content/drive/Shareddrives/monitor-rosa-silver/databases/ibge_silver.db/cadastro_municipios'\n",
            "Tabela demografia_municipios criada\n",
            "Tabela demografia_municipios criada com comando CREATE TABLE IF NOT EXISTS ibge_silver.demografia_municipios USING delta LOCATION '/content/drive/Shareddrives/monitor-rosa-silver/databases/ibge_silver.db/demografia_municipios'\n",
            "Recriação das tabelas concluída.\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "zone_names = ['monitor-rosa-bronze','monitor-rosa-silver','monitor-rosa-gold']\n",
        "\n",
        "if datalake_mode in ('leitura', 'escrita'):\n",
        "\n",
        "    zone_paths = [f'/content/datalake/{zone_name}/databases' for zone_name in zone_names]\n",
        "else:\n",
        "    zone_paths = [f'/content/drive/Shareddrives/{zone_name}/databases' for zone_name in zone_names]\n",
        "\n",
        "\n",
        "\n",
        "# Carrega catalogo de banco de dados, na zona bronze\n",
        "database_filter = None #['cnes_bronze.db',]\n",
        "\n",
        "table_filter = None #['sia_bronze.ar','sia_bronze.aq', 'ibge_silver.cadastro_municipios', 'ibge_silver.demografia_municipios' ]\n",
        "\n",
        "for databases_path in zone_paths:\n",
        "    load_entire_catalog_fs_v2(\n",
        "        spark_session = spark,\n",
        "        databases_path = databases_path,\n",
        "        use_db_folder_path=(datalake_mode == 'escrita'),\n",
        "        database_filter=database_filter,\n",
        "        table_filter=table_filter\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaP8BGb8HTqL"
      },
      "source": [
        "## 1.9 - Cria banco de dados gold\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "H641SkVm6iQk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "459ce6ec-c6f9-4678-a2fc-3d2ed590501e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Banco de dados cancer_mama_1 criado.\n"
          ]
        }
      ],
      "source": [
        "destination_database_name = 'cancer_mama_1'\n",
        "\n",
        "warehouse_dir_g =  f\"/content/datalake/{lake_prefix}/\"\n",
        "\n",
        "\n",
        "db_creator = DeltaLakeDatabaseFsCreator(\n",
        "    spark_session= spark,\n",
        "    database_location=warehouse_dir_g,\n",
        "    database_name=destination_database_name\n",
        ")\n",
        "db_creator.create_database(\n",
        "     use_db_folder_path=(datalake_mode == 'escrita')\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeQru0rTIfct"
      },
      "source": [
        "# II - Exemplo de como listar bancos e tabelas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "qJybgzFHU5as",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2b04ee3-7a71-474a-f3ce-aaf81e26a4a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+\n",
            "|  namespace|\n",
            "+-----------+\n",
            "|cancer_data|\n",
            "|cancer_mama|\n",
            "|cnes_bronze|\n",
            "|    default|\n",
            "|ibge_silver|\n",
            "| sia_bronze|\n",
            "| sih_bronze|\n",
            "| sim_bronze|\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "databases = spark.sql(f\"SHOW DATABASES;\")\n",
        "databases.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYbmd2NNH3iZ",
        "outputId": "690c9e32-2bb3-477e-f001-7294f89e15e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------------------+-----------+\n",
            "|namespace  |tableName                |isTemporary|\n",
            "+-----------+-------------------------+-----------+\n",
            "|cancer_data|aq_filtered              |false      |\n",
            "|cancer_data|ar_filtered              |false      |\n",
            "|cancer_data|cadastro_municipios      |false      |\n",
            "|cancer_data|dados_estados_mensal     |false      |\n",
            "|cancer_data|dados_municipios_mensal  |false      |\n",
            "|cancer_data|demografia_municipios    |false      |\n",
            "|cancer_data|pacientes                |false      |\n",
            "|cancer_data|procedimentos            |false      |\n",
            "|cancer_data|procedimentos_e_pacientes|false      |\n",
            "+-----------+-------------------------+-----------+\n",
            "\n",
            "+---------+---------+-----------+\n",
            "|namespace|tableName|isTemporary|\n",
            "+---------+---------+-----------+\n",
            "+---------+---------+-----------+\n",
            "\n",
            "+-----------+---------+-----------+\n",
            "|namespace  |tableName|isTemporary|\n",
            "+-----------+---------+-----------+\n",
            "|cnes_bronze|dc       |false      |\n",
            "|cnes_bronze|ep       |false      |\n",
            "|cnes_bronze|eq       |false      |\n",
            "|cnes_bronze|hb       |false      |\n",
            "|cnes_bronze|in       |false      |\n",
            "|cnes_bronze|lt       |false      |\n",
            "|cnes_bronze|pf       |false      |\n",
            "|cnes_bronze|rc       |false      |\n",
            "|cnes_bronze|sr       |false      |\n",
            "+-----------+---------+-----------+\n",
            "\n",
            "+---------+---------+-----------+\n",
            "|namespace|tableName|isTemporary|\n",
            "+---------+---------+-----------+\n",
            "+---------+---------+-----------+\n",
            "\n",
            "+-----------+---------------------+-----------+\n",
            "|namespace  |tableName            |isTemporary|\n",
            "+-----------+---------------------+-----------+\n",
            "|ibge_silver|cadastro_municipios  |false      |\n",
            "|ibge_silver|demografia_municipios|false      |\n",
            "+-----------+---------------------+-----------+\n",
            "\n",
            "+----------+---------+-----------+\n",
            "|namespace |tableName|isTemporary|\n",
            "+----------+---------+-----------+\n",
            "|sia_bronze|am       |false      |\n",
            "|sia_bronze|aq       |false      |\n",
            "|sia_bronze|ar       |false      |\n",
            "|sia_bronze|bi       |false      |\n",
            "|sia_bronze|pa       |false      |\n",
            "+----------+---------+-----------+\n",
            "\n",
            "+----------+---------+-----------+\n",
            "|namespace |tableName|isTemporary|\n",
            "+----------+---------+-----------+\n",
            "|sih_bronze|rd       |false      |\n",
            "+----------+---------+-----------+\n",
            "\n",
            "+----------+---------+-----------+\n",
            "|namespace |tableName|isTemporary|\n",
            "+----------+---------+-----------+\n",
            "|sim_bronze|dores    |false      |\n",
            "+----------+---------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for row in databases.collect():\n",
        "    spark.sql(f\"SHOW TABLES FROM {row['namespace']};\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jy88IuwDyjFQ"
      },
      "source": [
        "# III - Extração e Filtragem:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa-XADxhEWvE"
      },
      "source": [
        "## 3.1 - Definindo utils e filtros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "_joP1K-LEbIQ"
      },
      "outputs": [],
      "source": [
        "# Função para gerar consultas SQL\n",
        "def get_select_all_query(table_name, where_clause=''):\n",
        "    return f\"\"\"\n",
        "    SELECT\n",
        "        *\n",
        "    FROM {table_name}\n",
        "    {where_clause}\n",
        "    \"\"\"\n",
        "\n",
        "# Função para executar consultas SQL no Spark\n",
        "def run_sql_query(sql_query):\n",
        "    return spark.sql(sql_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rfb1iT7fEnCh",
        "outputId": "d5774ab7-1606-4649-afd9-733502c17581"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('0201010569','0201010585','0201010607','0203010035','0203010043','0203020065','0203020073','0205020097','0208090037','0204030030','0204030188')\n",
            "('C500','C501','C502','C503','C504','C505','C506','C508','C509')\n"
          ]
        }
      ],
      "source": [
        "# Definindo variavel destination_database_name\n",
        "\n",
        "destination_database_name = 'cancer_mama'\n",
        "\n",
        "# Filtro pelo CID\n",
        "\n",
        "cid_filter = ['C500', 'C501', 'C502', 'C503', 'C504', 'C505', 'C506', 'C508', 'C509']\n",
        "\n",
        "cid_filter = f\"\"\"({','.join([f\"'{cid_id}'\" for cid_id in cid_filter])})\"\"\"\n",
        "\n",
        "# Dicionário e filtro de procedimentos\n",
        "proc_id_dict = {\n",
        "    '0201010569': 'BIOPSIA/EXERESE DE NÓDULO DE MAMA',\n",
        "    '0201010585': 'PUNÇÃO ASPIRATIVA DE MAMA POR AGULHA FINA',\n",
        "    '0201010607': 'PUNÇÃO DE MAMA POR AGULHA GROSSA',\n",
        "    '0203010035': 'EXAME DE CITOLOGIA (EXCETO CERVICO-VAGINAL E DE MAMA)',\n",
        "    '0203010043': 'EXAME CITOPATOLOGICO DE MAMA',\n",
        "    '0203020065': 'EXAME ANATOMOPATOLOGICO DE MAMA - BIOPSIA',\n",
        "    '0203020073': 'EXAME ANATOMOPATOLOGICO DE MAMA - PECA CIRURGICA',\n",
        "    '0205020097': 'ULTRASSONOGRAFIA MAMARIA BILATERAL',\n",
        "    '0208090037': 'CINTILOGRAFIA DE MAMA (BILATERAL)',\n",
        "    '0204030030': 'MAMOGRAFIA',\n",
        "    '0204030188': 'MAMOGRAFIA BILATERAL PARA RASTREAMENTO'\n",
        "}\n",
        "\n",
        "proc_id_filter = f\"\"\"({','.join([f\"'{proc_id}'\" for proc_id in proc_id_dict.keys()])})\"\"\"\n",
        "\n",
        "# Mostrar os filtros\n",
        "print(proc_id_filter)\n",
        "print(cid_filter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-9lMS3vug83"
      },
      "source": [
        "## 3.2 - Carrega Tabela de Cadastro de Municípios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "T_zDu3JoujjE"
      },
      "outputs": [],
      "source": [
        "query_cadastro_municipios = spark.sql(\"\"\"\n",
        "\n",
        "SELECT *\n",
        "FROM\n",
        "ibge_silver.cadastro_municipios\n",
        "\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeIwwmsSHTzv"
      },
      "source": [
        "## 3.2 - Carrega tabela SIA.AR filtrando dados de câncer de mama e procedimentos de interesse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GoyzZRm7NUw",
        "outputId": "32fafdef-f6ad-4953-c539-68505f7c4340"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------+---------+---------+-------------+------+----------+--------+--------+--------+---------+---------+--------------+-------------+---------------+----------+----------+-------+----------+---------+----------+---------+--------+--------+---------+--------+---------+---------+---------+--------+---------+---------+-------+---------+---------+----------+---------+-------------+---------+----------+--------+---------+---------+---------+--------+-------+--------+---------+---------+---------+---------+---------+----------+---------+----------+---------+----------+---------+---------+---------+---------+---------+---------+---------+--------+---------+---------+---------+---------+---------+---------+--------+--------+--------------------+----------------------+--------------------+-------------------+---------+\n",
            "|AP_MVM|AP_CONDIC|AP_GESTAO|AP_CODUNI|   AP_AUTORIZ|AP_CMP| AP_PRIPAL|AP_VL_AP|AP_UFMUN|AP_TPUPS|AP_TIPPRE|AP_MN_IND|    AP_CNPJCPF|   AP_CNPJMNT|      AP_CNSPCN|AP_COIDADE|AP_NUIDADE|AP_SEXO|AP_RACACOR|AP_MUNPCN|AP_UFNACIO|AP_CEPPCN|AP_UFDIF|AP_MNDIF|AP_DTINIC|AP_DTFIM|AP_TPATEN|AP_TPAPAC|AP_MOTSAI|AP_OBITO|AP_ENCERR|AP_PERMAN|AP_ALTA|AP_TRANSF|AP_DTOCOR| AP_CODEMI|AP_CATEND|   AP_APACANT|AP_UNISOL|AP_DTSOLIC|AP_DTAUT|AP_CIDCAS|AP_CIDPRI|AP_CIDSEC|AP_ETNIA|AR_SMRD|AR_CID10|AR_LINFIN|AR_ESTADI|AR_GRAHIS|AR_DTIDEN|AR_TRANTE|AR_CIDINI1|AR_DTINI1|AR_CIDINI2|AR_DTINI2|AR_CIDINI3|AR_DTINI3|AR_CONTTR|AR_DTINTR|AR_FINALI|AR_CIDTR1|AR_CIDTR2|AR_CIDTR3|AR_NUMC1|AR_INIAR1|AR_INIAR2|AR_INIAR3|AR_FIMAR1|AR_FIMAR2|AR_FIMAR3|AR_NUMC2|AR_NUMC3|           _filepath|file_modification_time|           _filename|   _update_datetime|AP_NATJUR|\n",
            "+------+---------+---------+---------+-------------+------+----------+--------+--------+--------+---------+---------+--------------+-------------+---------------+----------+----------+-------+----------+---------+----------+---------+--------+--------+---------+--------+---------+---------+---------+--------+---------+---------+-------+---------+---------+----------+---------+-------------+---------+----------+--------+---------+---------+---------+--------+-------+--------+---------+---------+---------+---------+---------+----------+---------+----------+---------+----------+---------+---------+---------+---------+---------+---------+---------+--------+---------+---------+---------+---------+---------+---------+--------+--------+--------------------+----------------------+--------------------+-------------------+---------+\n",
            "|202405|       EP|   350000|  2077485|3524250522122|202405|0304010413|  5904.0|  355030|      05|       00|        I|61699567000192|0000000000000|{{|}{}}}|         4|        72|      F|        01|   355030|       010| 04081001|       0|       1| 20240503|20240531|       04|        3|       15|       0|        0|        0|      1|        0| 20240524|E350000027|       01|0000000000000|  2077485|  20240503|20240503|     0000|     C509|     0000|        |       |        |        3|        1|        0| 20240207|        N|          |         |          |         |          |         |        N| 20240503|        2|     C509|         |         |        | 20240503|         |         | 20240510|         |         |        |        |file:///home/runn...|   2024-10-09 11:58:01|ARSP2405.parquet....|2024-10-10 00:59:17|     3999|\n",
            "|202405|       EP|   350000|  2077477|3524249650427|202405|0304010413|  5904.0|  355030|      05|       00|        I|60742616000160|0000000000000|{{}}}}~~~|         4|        60|      F|        03|   352310|       010| 08573160|       0|       1| 20240508|20240630|       04|        3|       18|       0|        0|        0|      1|        0| 20240508|E350000027|       01|0000000000000|  2077477|  20240508|20240508|     0000|     C509|     0000|        |       |    C509|        3|        3|       02| 20230815|        S|      C509| 20231101|      C509| 20240313|          |         |        N| 20240508|        2|     C509|         |         |        | 20240508|         |         | 20240731|         |         |        |        |file:///home/runn...|   2024-10-09 11:58:01|ARSP2405.parquet....|2024-10-10 00:59:17|     3999|\n",
            "|202405|       EP|   350000|  2077485|3524242871545|202404|0304010413|     0.0|  355030|      05|       00|        I|61699567000192|0000000000000|{|{}||         4|        79|      F|        01|   355030|       010| 08490600|       0|       1| 20240401|20240430|       04|        3|       15|       0|        0|        0|      1|        0| 20240430|E350000027|       01|0000000000000|  2077485|  20240401|20240401|     0000|     C509|     0000|        |       |        |        S|        2|        0| 20230517|        N|          |         |          |         |          |         |        N| 20240401|        5|     C509|         |         |        | 20240401|         |         | 20240630|         |         |        |        |file:///home/runn...|   2024-10-09 11:58:01|ARSP2405.parquet....|2024-10-10 00:59:17|     3999|\n",
            "+------+---------+---------+---------+-------------+------+----------+--------+--------+--------+---------+---------+--------------+-------------+---------------+----------+----------+-------+----------+---------+----------+---------+--------+--------+---------+--------+---------+---------+---------+--------+---------+---------+-------+---------+---------+----------+---------+-------------+---------+----------+--------+---------+---------+---------+--------+-------+--------+---------+---------+---------+---------+---------+----------+---------+----------+---------+----------+---------+---------+---------+---------+---------+---------+---------+--------+---------+---------+---------+---------+---------+---------+--------+--------+--------------------+----------------------+--------------------+-------------------+---------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sql_query_ar = get_select_all_query(\n",
        "    table_name='sia_bronze.ar',\n",
        "    where_clause=f\"\"\"\n",
        "\n",
        "        WHERE AP_CIDPRI IN {cid_filter}\n",
        "        AND AP_MVM = '202405'\n",
        "        \"\"\"\n",
        ")\n",
        "\n",
        "cancer_ar_filtered = run_sql_query(sql_query_ar)\n",
        "\n",
        "\n",
        "cancer_ar_filtered.show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "HLCBiUZQ2R2t"
      },
      "outputs": [],
      "source": [
        "cancer_ar_filtered\\\n",
        "      .repartition(1)\\\n",
        "      .write\\\n",
        "      .format(\"delta\")\\\n",
        "      .mode(\"overwrite\")\\\n",
        "      .saveAsTable(f\"{destination_database_name}.ar_filtered\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjhvBzKogaT9"
      },
      "source": [
        "## 3.3 - Carrega tabela SIA.AQ filtrando dados de câncer de mama e procedimentos de interesse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bIQolpgc_a9"
      },
      "outputs": [],
      "source": [
        "sql_query_aq = get_select_all_query(\n",
        "    table_name='sia_bronze.aq',\n",
        "    where_clause=f\"\"\"\n",
        "        WHERE AP_CIDPRI IN {cid_filter}\n",
        "        AND AP_MVM = '202405'\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "cancer_aq_filtered = run_sql_query(sql_query_aq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-Jga0Lpeq32"
      },
      "outputs": [],
      "source": [
        "cancer_aq_filtered\\\n",
        "      .repartition(1)\\\n",
        "      .write\\\n",
        "      .format(\"delta\")\\\n",
        "      .mode(\"overwrite\")\\\n",
        "      .saveAsTable(f\"{destination_database_name}.aq_filtered\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ncymjIK6NlR"
      },
      "source": [
        "# IV - Processamento dos Dados dos Pacientes e Procedimentos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rn0xcaY-j3E2"
      },
      "source": [
        "## 4.1 - Cria dados consolidados de pacientes e procedimentos (quimio e radioterapia)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mx39mQ5w4S6"
      },
      "source": [
        "Radioterapia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozvlA8On_dHJ",
        "outputId": "bcd0ab1d-6a8b-4a20-ab57-e050a00e97a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+---------------+------------+------+-----+---------+\n",
            "|  data|       paciente|estadiamento| custo|obito|municipio|\n",
            "+------+---------------+------------+------+-----+---------+\n",
            "|202405|{{|}{}}}|           1|5904.0|    0|   355030|\n",
            "|202405|{{}}}}~~~|           3|5904.0|    0|   352310|\n",
            "|202404|{|{}||           2|   0.0|    0|   355030|\n",
            "+------+---------------+------------+------+-----+---------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "cancer_ar_res = spark.sql(f\"\"\"\n",
        "SELECT\n",
        "    AP_CMP as data,\n",
        "    AP_CNSPCN as paciente,\n",
        "    AR_ESTADI as estadiamento,\n",
        "    DOUBLE(AP_VL_AP)  as custo,\n",
        "    INT(AP_OBITO) as obito,\n",
        "    AP_MUNPCN as municipio\n",
        "FROM cancer_data.ar_filtered\n",
        "\"\"\")\n",
        "cancer_ar_res.show(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlF0pvjRw74j"
      },
      "source": [
        "Quimio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Dyzqz4rGhmG",
        "outputId": "4f04274d-c369-4a8b-e3d3-5757c6f0a8d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+---------------+------------+------+-----+---------+\n",
            "|  data|       paciente|estadiamento| custo|obito|municipio|\n",
            "+------+---------------+------------+------+-----+---------+\n",
            "|202404|{{}{|{|           3|1400.0|    0|   351640|\n",
            "|202404|{{||~}||           3|1400.0|    0|   355030|\n",
            "|202405|{}~{|{||||           4|1400.0|    0|   355220|\n",
            "+------+---------------+------------+------+-----+---------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "cancer_aq_res = spark.sql(f\"\"\"\n",
        "SELECT\n",
        "    AP_CMP as data,\n",
        "    AP_CNSPCN as paciente,\n",
        "    AQ_ESTADI as estadiamento,\n",
        "    DOUBLE(AP_VL_AP)  as custo,\n",
        "    INT(AP_OBITO) as obito,\n",
        "    AP_MUNPCN as municipio\n",
        "FROM cancer_data.aq_filtered\n",
        "\"\"\")\n",
        "cancer_aq_res.show(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOgAevcCNa07"
      },
      "source": [
        "## 4.2 - Unifica os dados de radio e quimio consolidados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94M3aPpRNZeT"
      },
      "outputs": [],
      "source": [
        "df_union = cancer_aq_res.union(cancer_ar_res)\n",
        "\n",
        "df_union.createOrReplaceTempView(\"cancer_ordered\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYZcjpFJVtbT"
      },
      "outputs": [],
      "source": [
        "df_union\\\n",
        "  .repartition(1)\\\n",
        "  .write\\\n",
        "  .format(\"delta\")\\\n",
        "  .mode(\"overwrite\")\\\n",
        "  .saveAsTable(f\"{destination_database_name}.procedimentos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s588c0EgN5pU"
      },
      "source": [
        "## 4.3 - Consolidando os dados por paciente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kymNuaRsg87T"
      },
      "outputs": [],
      "source": [
        "res_consolidado = spark.sql(\"\"\"\n",
        "SELECT\n",
        "    paciente,\n",
        "    FIRST(data) as data_primeiro_estadiamento,\n",
        "    LAST(data) as data_ultimo_estadiamento,\n",
        "    COUNT(1) as numero_procedimentos,\n",
        "    FIRST(estadiamento) as primeiro_estadiamento,\n",
        "    LAST(estadiamento) as ultimo_estadiamento,\n",
        "    MAX (estadiamento) as maior_estadiamento,\n",
        "    MIN (estadiamento) as menor_estadiamento,\n",
        "    SUM(custo) as custo_total,\n",
        "    MAX(obito) as indicacao_obito,\n",
        "    FIRST(municipio) as primeiro_municipio,\n",
        "    LAST(municipio) as ultimo_municipio\n",
        "FROM (SELECT * FROM cancer_ordered ORDER BY paciente, data)\n",
        "GROUP BY paciente\n",
        "\"\"\")\n",
        "\n",
        "res_consolidado\\\n",
        "  .repartition(1)\\\n",
        "  .write\\\n",
        "  .format(\"delta\")\\\n",
        "  .mode(\"overwrite\")\\\n",
        "  .saveAsTable(f\"{destination_database_name}.pacientes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31EFS5RgWrPr"
      },
      "source": [
        "## 4.4 - Procedimentos e Pacientes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2319Trei2iY"
      },
      "outputs": [],
      "source": [
        "procedimentos_e_pacientes = spark.sql(f\"\"\"\n",
        "  SELECT\n",
        "      c.*,\n",
        "      p.data_primeiro_estadiamento,\n",
        "      p.data_ultimo_estadiamento,\n",
        "      p.primeiro_estadiamento,\n",
        "      p.maior_estadiamento,\n",
        "      p.ultimo_estadiamento,\n",
        "      p.custo_total,\n",
        "      p.primeiro_municipio,\n",
        "      p.ultimo_municipio,\n",
        "      p.indicacao_obito\n",
        "  FROM cancer_data.procedimentos AS c\n",
        "  FULL OUTER JOIN cancer_data.pacientes AS p\n",
        "  ON c.paciente = p.paciente\n",
        "\"\"\")\n",
        "\n",
        "procedimentos_e_pacientes\\\n",
        "  .repartition(1)\\\n",
        "  .write\\\n",
        "  .format(\"delta\")\\\n",
        "  .mode(\"overwrite\")\\\n",
        "  .saveAsTable(f\"{destination_database_name}.procedimentos_e_pacientes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZyJAon8CaMw"
      },
      "source": [
        "# V - Agregação por Município e Estado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRM6BlAT9xPY"
      },
      "source": [
        "## 5.1 - Consolida dados por municipio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60A9MQRpaCZk"
      },
      "outputs": [],
      "source": [
        "database_name = \"cancer_data\"\n",
        "\n",
        "diagnosticos_por_estadiamento_municipio_df = spark.sql(f\"\"\"\n",
        "    SELECT\n",
        "        primeiro_estadiamento,\n",
        "        data_primeiro_estadiamento AS data,\n",
        "        primeiro_municipio AS municipio,\n",
        "        COUNT(DISTINCT(paciente)) AS numero_diagnosticos\n",
        "    FROM {destination_database_name}.pacientes\n",
        "    WHERE primeiro_estadiamento != ''\n",
        "    GROUP BY primeiro_estadiamento, data_primeiro_estadiamento, primeiro_municipio\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "\n",
        "diagnosticos_por_estadiamento_municipio_df.createOrReplaceTempView(\"diagnosticos_por_estadiamento_municipio\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbZFX_cNdwPt"
      },
      "source": [
        "## 5.2 - Consolida dados mensais por municipio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLwtdxLCdrTN"
      },
      "outputs": [],
      "source": [
        "dados_estad_municipio_mensal_df = spark.sql(f\"\"\"\n",
        "    SELECT\n",
        "        data,\n",
        "        municipio,\n",
        "        primeiro_estadiamento,\n",
        "        SUM(custo) AS custo_estadiamento,\n",
        "        COUNT(DISTINCT(paciente)) AS numero_pacientes,\n",
        "        SUM(DISTINCT(obito)) AS obitos,\n",
        "        SUM(DISTINCT(indicacao_obito)) AS obito_futuro,\n",
        "        COUNT(1) AS numero_procedimentos\n",
        "    FROM\n",
        "        (SELECT * FROM {destination_database_name}.procedimentos_e_pacientes ORDER BY data)\n",
        "    GROUP BY data, municipio, primeiro_estadiamento\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "\n",
        "dados_estad_municipio_mensal_df.createOrReplaceTempView(\"dados_municipios_mensal\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mS4PmtP9dFfD"
      },
      "source": [
        "## 5.3 - Consolida dados por municipio mensal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQBFWXwT9ts3"
      },
      "outputs": [],
      "source": [
        "dados_estad_municipio_mensal = spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        mm.*,\n",
        "        COALESCE(em.numero_diagnosticos, 0) AS numero_diagnosticos\n",
        "    FROM dados_municipios_mensal mm\n",
        "    FULL OUTER JOIN diagnosticos_por_estadiamento_municipio em\n",
        "    ON mm.data = em.data\n",
        "    AND mm.municipio = em.municipio\n",
        "    AND mm.primeiro_estadiamento = em.primeiro_estadiamento\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "\n",
        "dados_estad_municipio_mensal\\\n",
        "    .repartition(1)\\\n",
        "    .write\\\n",
        "    .format(\"delta\")\\\n",
        "    .mode(\"overwrite\")\\\n",
        "    .saveAsTable(f\"{destination_database_name}.dados_estad_municipio_mensal\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1z0tYQaderL1"
      },
      "source": [
        "## 5.4 - Agregação por estado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "id": "x3Ddz0QveqKk",
        "outputId": "f169baa4-27e8-43f6-f971-1abb2b9fc3a5"
      },
      "outputs": [
        {
          "ename": "AnalysisException",
          "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `cancer_data`.`dados_municipios_mensal` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 17 pos 13;\n'Aggregate ['estado, 'data, 'primeiro_estadiamento], ['estado, 'data, 'primeiro_estadiamento, 'SUM('custo_estadiamento) AS custo_estadiamento#9639, 'SUM('numero_pacientes) AS numero_pacientes#9640, 'COUNT(distinct 'municipio) AS numero_municipios#9641, 'SUM('obitos) AS obitos#9642, 'SUM('obito_futuro) AS obitos_futuros#9643, 'SUM('numero_procedimentos) AS numero_procedimentos#9644, 'SUM('numero_diagnosticos) AS numero_diagnosticos#9645]\n+- 'SubqueryAlias dados_estado\n   +- 'Sort ['data ASC NULLS FIRST], true\n      +- 'Project ['cadastro_cidades.nome_uf AS estado#9638, mm.*]\n         +- 'Join LeftOuter, ('int('mm.municipio) = 'int(('cadastro_cidades.id / 10)))\n            :- 'SubqueryAlias mm\n            :  +- 'UnresolvedRelation [cancer_data, dados_municipios_mensal], [], false\n            +- SubqueryAlias cadastro_cidades\n               +- SubqueryAlias spark_catalog.cancer_data.cadastro_municipios\n                  +- Relation spark_catalog.cancer_data.cadastro_municipios[id#9646,nome#9647,id_uf#9648,nome_uf#9649] parquet\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-51ed15e56d54>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m dados_estad_mensal = spark.sql(f\"\"\"\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mSELECT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mestado\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprimeiro_estadiamento\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1438\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1439\u001b[0m             \u001b[0mlitArgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_to_java_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1440\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlitArgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1441\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `cancer_data`.`dados_municipios_mensal` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 17 pos 13;\n'Aggregate ['estado, 'data, 'primeiro_estadiamento], ['estado, 'data, 'primeiro_estadiamento, 'SUM('custo_estadiamento) AS custo_estadiamento#9639, 'SUM('numero_pacientes) AS numero_pacientes#9640, 'COUNT(distinct 'municipio) AS numero_municipios#9641, 'SUM('obitos) AS obitos#9642, 'SUM('obito_futuro) AS obitos_futuros#9643, 'SUM('numero_procedimentos) AS numero_procedimentos#9644, 'SUM('numero_diagnosticos) AS numero_diagnosticos#9645]\n+- 'SubqueryAlias dados_estado\n   +- 'Sort ['data ASC NULLS FIRST], true\n      +- 'Project ['cadastro_cidades.nome_uf AS estado#9638, mm.*]\n         +- 'Join LeftOuter, ('int('mm.municipio) = 'int(('cadastro_cidades.id / 10)))\n            :- 'SubqueryAlias mm\n            :  +- 'UnresolvedRelation [cancer_data, dados_municipios_mensal], [], false\n            +- SubqueryAlias cadastro_cidades\n               +- SubqueryAlias spark_catalog.cancer_data.cadastro_municipios\n                  +- Relation spark_catalog.cancer_data.cadastro_municipios[id#9646,nome#9647,id_uf#9648,nome_uf#9649] parquet\n"
          ]
        }
      ],
      "source": [
        "dados_estad_mensal = spark.sql(f\"\"\"\n",
        "    SELECT\n",
        "        estado,\n",
        "        data,\n",
        "        primeiro_estadiamento,\n",
        "        SUM(custo_estadiamento) AS custo_estadiamento,\n",
        "        SUM(numero_pacientes) AS numero_pacientes,\n",
        "        COUNT(DISTINCT(municipio)) AS numero_municipios,\n",
        "        SUM(obitos) AS obitos,\n",
        "        SUM(obito_futuro) AS obitos_futuros,\n",
        "        SUM(numero_procedimentos) AS numero_procedimentos,\n",
        "        SUM(numero_diagnosticos) AS numero_diagnosticos\n",
        "    FROM (\n",
        "        SELECT\n",
        "            cadastro_cidades.nome_uf AS estado,\n",
        "            mm.*\n",
        "        FROM cancer_data.dados_municipios_mensal mm\n",
        "        LEFT JOIN cancer_data.cadastro_municipios AS cadastro_cidades\n",
        "        ON int(mm.municipio) = int(cadastro_cidades.id / 10)\n",
        "        ORDER BY data\n",
        "    ) AS dados_estado\n",
        "    GROUP BY estado, data, primeiro_estadiamento\n",
        "\"\"\")\n",
        "\n",
        "# Salvando a tabela agregada por estado\n",
        "dados_estad_mensal\\\n",
        "    .repartition(1)\\\n",
        "    .write\\\n",
        "    .format(\"delta\")\\\n",
        "    .mode(\"overwrite\")\\\n",
        "    .saveAsTable(f\"{destination_database_name}.dados_estados_mensal\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7YEI93j-zbA"
      },
      "source": [
        "# VI - Limpeza das tabelas delta, considerando 24 horas de retenção\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rafFyT9G-cFg"
      },
      "outputs": [],
      "source": [
        "vacuum_tables_from_database(\n",
        "        spark_session = spark,\n",
        "        database_name = database_name,\n",
        "        retention_hours = 24\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "9ncymjIK6NlR",
        "4ZyJAon8CaMw"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}