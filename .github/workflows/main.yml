name: Update analisys datasets
run-name: ${{ github.actor }} is updating analysis datasets üöÄ
on: 
  workflow_dispatch:
    inputs:  
      states:
        description: 'States to collect files'
        required: false
        default: 'SP'  
      start_date:
        description: 'Start date (YYYY-MM-DD)'
        required: false
      end_date:
        description: 'End date (YYYY-MM-DD)'
        required: false        
  schedule:
    - cron:  '0 8 5,6,7 * *'
    
jobs:
  Update-Datasets:
    runs-on: ubuntu-latest
    steps:
      - run: echo "üéâ The job was triggered by a ${{ github.event_name }} event."
      - run: echo "üêß This job is now running on a ${{ runner.os }} server hosted by GitHub!"
      - run: echo "üîé The name of your branch is ${{ github.ref }} and your repository is ${{ github.repository }}."
      - name: Check out repository code
        uses: actions/checkout@v3
      - run: echo "üí° The ${{ github.repository }} repository has been cloned to the runner."
      - run: echo "üñ•Ô∏è The workflow is now ready to test your code on the runner."
      - name: List files in the repository
        run: |
          ls ${{ github.workspace }}
      - name: 'google-auth'
        uses: 'google-github-actions/auth@v1'
        with:
          credentials_json: '${{ secrets.gcp_credentials }}'
          
      - name: Download and prepare Spark
        run: |           
          sudo apt-get update  > /dev/null
          sudo apt-get install openjdk-8-jdk-headless -qq > /dev/null
          sudo wget -q https://downloads.apache.org/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz
          sudo tar xf spark-3.4.1-bin-hadoop3.tgz
          pip3 install findspark pyspark imagehash
          pip3 install delta-spark==2.4.0
          
          sudo wget -q https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop3-latest.jar

          export PYTHONHASHSEED=1234
          export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
          export SPARK_HOME=./spark-3.4.1-bin-hadoop3
          export SPARK_VERSION=3.4.1
          
          sudo mv ./gcs-connector-hadoop3-latest.jar $SPARK_HOME/jars          

          echo "running script"
          python3 ./sia/etls/test-spark.py

      - name: Create output and temp dir
        run: |  
          export OUTPUT_DIR=${{ github.workspace }}/output-files
          export TEMP_DIR=${{ github.workspace }}/temp-files          
          mkdir ${OUTPUT_DIR}
          mkdir ${TEMP_DIR}
      - name: 'install gcsfuse to access bucket'
        run: |
          export GCSFUSE_REPO=gcsfuse-`lsb_release -c -s`
          echo "deb https://packages.cloud.google.com/apt $GCSFUSE_REPO main" | sudo tee /etc/apt/sources.list.d/gcsfuse.list
          curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
          sudo apt-get update
          sudo apt-get install gcsfuse
          sudo groupadd fuse
          sudo usermod -a -G fuse $USER
          sudo chown $USER_ID:fuse ${{ github.workspace }}/output-files
          
          gcsfuse --dir-mode=777 --file-mode=777 --key-file=$GOOGLE_APPLICATION_CREDENTIALS --implicit-dirs observatorio-oncologia ${{ github.workspace }}/output-files
          
      - name: Prepare permissions, scripts and call script to collect files
        working-directory: ./sia/etls
        run: |
          export INPUT_DIR=${{ github.workspace }}/output-files/monitor        
          export OUTPUT_DIR=${{ github.workspace }}/output-files/monitor
          export TEMP_DIR=${{ github.workspace }}/temp-files  

          export STATES=${{ github.event.inputs.states}}
          export START_DATE=${{ github.event.inputs.start_date}}
          export END_DATE=${{ github.event.inputs.end_date}}  
          echo "installing python packages"
          pip3 install -r ./requirements.txt
          echo "running script"
          python3 ./test.py

      - run: echo "üçè This job's status is ${{ job.status }}." 
      - uses: gautamkrishnar/keepalive-workflow@v1 # using the workflow with default settings
